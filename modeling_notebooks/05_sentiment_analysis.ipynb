{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the tweet-preprocessor package to clean our tweet text. We'll drop non-english tweets, and then use Stanford Core NLP for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "from string import punctuation\n",
    "import time\n",
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('../data/data_modified/tweets/tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT @SaveTheVikesOrg: Drumming up #Vikings stadium support @mnstatefair w/ @nicolelindaman and @laurabesh before concert! #countryfansare ...',\n",
       " 'RT @Haydollbaby: #steelersnation',\n",
       " 'RT @MyPhilaEagles: Yo the bol @DeseanJackson10 is toooo damnnnnn FAST on #Madden12 hahaha #Eagles #NFL',\n",
       " \"@DEZ_88....yo i'm headed to miami right after work tomorrow to u guys beat the dolphins!!!!#GOCOWBOYS\",\n",
       " 'RT @nfl: Tom Brady and @Ochocinco still looking for that chemistry: http://t.co/1IZPpQq #Patriots',\n",
       " \"@JermichaelF88 I'm drafting now, how you feeling? Going to have an awesome year? #gopackgo\",\n",
       " 'I wish the #Seahawks would switch back to their old school uniforms...they were #ballerstatus vintage is boss!',\n",
       " \"@ChrisJohnson28 #ifchrisjohnsonwaswhite he'd probably honor his contract and THEN get a new mega contract!!!!! #NFL #RaiderNation\",\n",
       " 'Falcons Shore Up Troubled Secondary In Two Short Days: The Falcoholic Â» \\xa0 Thomas Dimitroff he... http://t.co/cH6zwIR #nfl #ATL #falcons',\n",
       " 'RT @Jonathanstewar1: I know the #planking is done but I has to do one more for the #panthernation #NFL #TeamStewart28  http://t.co/KSkNnFW']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['content'][0:10].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 2s, sys: 393 ms, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean all the tweets, removing leading and lagging punctuation\n",
    "tweets['content_clean'] = [p.clean(tweet).strip(punctuation) for tweet in tweets['content'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 31min 59s, sys: 2min 33s, total: 1h 34min 32s\n",
      "Wall time: 1h 34min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Find how many tweets are not in English using langdetect\n",
    "langs = []\n",
    "for text in tweets['content_clean']:\n",
    "    try:\n",
    "        langs.append(detect(text))\n",
    "    except:\n",
    "        langs.append('')\n",
    "tweets['language'] = langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That was a long process, so let's save our progress\n",
    "tweets.to_csv('../data/data_modified/tweets/tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the data anecdotally, it looks like most non-English tweets that are not Spanish are either false positives or empty (i.e, only hashtags). We want to keep all tweets for count features. We only want to do sentiment analysis for non-Spanish, non-empty tweets.\n",
    "\n",
    "For the purposes of this model, we will treat all 'empty text' tweets as 'neutral' sentiment. This is contestable, however - anecdotal evidence suggests these may actually be positive (most are hashtag-only support for a given team). We won't dive into this much here, but it is a clear area for further research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Stanford Core NLP - must first download English version from https://stanfordnlp.github.io/CoreNLP/\n",
    "# and run this command locally in the unzipped folder\n",
    "\n",
    "# java -mx5g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -timeout 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 9min 58s, sys: 9min 13s, total: 1h 19min 12s\n",
      "Wall time: 18h 37min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Use NLP on the sentence level to determine overall sentiment of the tweet\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "\n",
    "sentiment_mappings = {\n",
    "    'Verypositive': 2,\n",
    "    'Positive': 1,\n",
    "    'Negative': -1,\n",
    "    'Verynegative': -2,\n",
    "    'Neutral': 0\n",
    "}\n",
    "sentiments = []\n",
    "\n",
    "for i, tweet in enumerate(tweets['content_clean']):\n",
    "    parsed_tweet = nlp.annotate(tweet,\n",
    "                       properties={\n",
    "                           'annotators': 'sentiment',\n",
    "                           'outputFormat': 'json',\n",
    "                           'timeout': 100000,\n",
    "                       })\n",
    "    sentiment = 0\n",
    "    for s in parsed_tweet[\"sentences\"]:\n",
    "        sentiment += sentiment_mappings[s[\"sentiment\"]]\n",
    "    sentiments.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sentiment'] = sentiments\n",
    "tweets['sentiment'] = [0 if sent[0]=='es' else sent[1] for sent in zip(tweets['language'], tweets['sentiment'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('../data/data_modified/tweets/tweets.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_fantasy",
   "language": "python",
   "name": "venv_fantasy"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
